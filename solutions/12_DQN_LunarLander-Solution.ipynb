{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCLrRFHSKl_5"
   },
   "source": [
    "# Deep Q-Network with Lunar Lander\n",
    "\n",
    "This notebook shows an implementation of a DQN on the LunarLander environment.\n",
    "Details on the environment can be found [here](https://gym.openai.com/envs/LunarLander-v2/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2RNqaAGiLU1"
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "We first need to install some dependencies for using the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96dExX1TKm2m",
    "outputId": "4a3a6d38-7cae-4c85-dc5f-c0182c2e8275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: box2d-py in /opt/miniconda3/envs/dmml2/lib/python3.8/site-packages (2.3.8)\n",
      "Requirement already satisfied: pyglet in /opt/miniconda3/envs/dmml2/lib/python3.8/site-packages (1.5.26)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install box2d-py # for mac\n",
    "# !pip3 install box2d-py # for windows\n",
    "!pip3 install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CZXskDwXKl_-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from time import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVO0INWR1DYS",
    "outputId": "68e453b9-79a7-4921-ac7c-186de15ec6c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f938b27f3f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lrq9VwzO1Zx4",
    "outputId": "ae7034e1-9628-4794-bba6-f7dac7ad5de4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKg3BvSnivPE"
   },
   "source": [
    "## 2. Define the neural network, the replay buffer and the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9pG_Ii7jToR"
   },
   "source": [
    "First, we define the neural network that predicts the Q-values for all actions, given a state as input.\n",
    "This is a fully-connected neural net with two hidden layers using Relu activations.\n",
    "The last layer does not have any activation and outputs a Q-value for every action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JFxqeLkf1eHY"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0xHaPTIj1pD"
   },
   "source": [
    "Next, we define a replay buffer that saves previous transitions and provides a `sample` function to randomly extract a batch of experiences from the buffer.\n",
    "\n",
    "Note that experiences are internally saved as `numpy`-arrays. They are converted back to PyTorch tensors before being returned by the `sample`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gQw6QVAC1hQf"
   },
   "outputs": [],
   "source": [
    "class StateTransition:\n",
    "    def __init__(self, state, action, reward, next_state, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.done = 1 if done else 0 # Convert done flag from boolean to int\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "       \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        state_transition = StateTransition(state, action, reward, next_state, done)\n",
    "        self.memory.append(state_transition)\n",
    "                \n",
    "    def sample(self):\n",
    "        state_transitions = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = np.vstack([s_t.state for s_t in state_transitions])\n",
    "        states_tensor = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        actions = np.vstack([s_t.action for s_t in state_transitions])\n",
    "        actions_tensor = torch.from_numpy(actions).long().to(device)\n",
    "\n",
    "        rewards = np.vstack([s_t.reward for s_t in state_transitions])\n",
    "        rewards_tensor = torch.from_numpy(rewards).float().to(device)\n",
    "\n",
    "        next_states = np.vstack([s_t.next_state for s_t in state_transitions])\n",
    "        next_states_tensor = torch.from_numpy(next_states).float().to(device)\n",
    "        \n",
    "        dones = np.vstack([s_t.done for s_t in state_transitions])\n",
    "        dones_tensor = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        return (states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor)\n",
    "        \n",
    "    def is_filled(self):\n",
    "        return len(self.memory) >= BATCH_SIZE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IYjlS7Fy1jJA"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000    # Replay memory size\n",
    "BATCH_SIZE = 64         # Number of experiences to sample from memory\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TARGET_SYNC = 20        # How often the target networks is synchronized\n",
    "       \n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Initialize Q and Target Q networks\n",
    "        self.q_network = QNetwork(state_size, action_size).to(device)\n",
    "        self.target_network = QNetwork(state_size, action_size).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        \n",
    "        # Initiliase replay buffer \n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        self.timestep = 0\n",
    "    \n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.timestep += 1\n",
    "        \n",
    "        if not self.memory.is_filled(): # train only when buffer is filled\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "               \n",
    "        # you need to implement the following method in task 5\n",
    "        loss = self.calculate_loss(states, actions, rewards, next_states, dones) \n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Synchronize target network by copying weights\n",
    "        if self.timestep % TARGET_SYNC == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    \n",
    "    def calculate_loss(self, states, actions, rewards, next_states, dones):\n",
    "    \n",
    "        action_values = self.target_network(next_states).detach()\n",
    "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # If \"done==1\" just use reward, else update Q_target with discounted action values\n",
    "        Q_target = rewards + (GAMMA * max_action_values * (1 - dones))\n",
    "        Q_prediction = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        # Calculate loss and update weights\n",
    "        loss = F.mse_loss(Q_prediction, Q_target)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def choose_action(self, state, epsilon):\n",
    "        rnd = random.random()\n",
    "        if rnd < epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action_values = self.q_network(state)\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P3-UIm0fh3W"
   },
   "source": [
    "### 3. Executes episodes and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NikAZhjNfsoi"
   },
   "source": [
    "We first define the necessary paramters for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lJGrZry81pu4"
   },
   "outputs": [],
   "source": [
    "TARGET_SCORE = 200            # Train until this score is reached\n",
    "MAX_EPISODE_LENGTH = 1000     # Max steps allowed in a single episode\n",
    "EPSILON_MIN = 0.01            # Minimum epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezOn9IpKf17C"
   },
   "source": [
    "Then we start executing episodes and observe the mean score per episode.\n",
    "The environment is considered as solved if this score is above 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_EC7XLJ1slY",
    "outputId": "ad110ade-36e5-4600-fcf0-faf2edc07235",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 8, action size: 4\n",
      "After 10 episodes, average score is -189.71. Took 3 seconds.\n",
      "After 20 episodes, average score is -187.55. Took 4 seconds.\n",
      "After 30 episodes, average score is -177.34. Took 14 seconds.\n",
      "After 40 episodes, average score is -155.51. Took 21 seconds.\n",
      "After 50 episodes, average score is -150.81. Took 17 seconds.\n",
      "After 60 episodes, average score is -135.88. Took 18 seconds.\n",
      "After 70 episodes, average score is -114.35. Took 17 seconds.\n",
      "After 80 episodes, average score is -99.03. Took 20 seconds.\n",
      "After 90 episodes, average score is -88.96. Took 22 seconds.\n",
      "After 100 episodes, average score is -82.97. Took 16 seconds.\n",
      "After 110 episodes, average score is -68.24. Took 21 seconds.\n",
      "After 120 episodes, average score is -43.03. Took 14 seconds.\n",
      "After 130 episodes, average score is -32.91. Took 13 seconds.\n",
      "After 140 episodes, average score is -39.19. Took 12 seconds.\n",
      "After 150 episodes, average score is -31.52. Took 26 seconds.\n",
      "After 160 episodes, average score is -37.27. Took 20 seconds.\n",
      "After 170 episodes, average score is -44.15. Took 23 seconds.\n",
      "After 180 episodes, average score is -47.45. Took 25 seconds.\n",
      "After 190 episodes, average score is -48.72. Took 25 seconds.\n",
      "After 200 episodes, average score is -40.01. Took 22 seconds.\n",
      "After 210 episodes, average score is -24.97. Took 20 seconds.\n",
      "After 220 episodes, average score is -29.05. Took 20 seconds.\n",
      "After 230 episodes, average score is -21.78. Took 23 seconds.\n",
      "After 240 episodes, average score is -5.60. Took 13 seconds.\n",
      "After 250 episodes, average score is 2.03. Took 12 seconds.\n",
      "After 260 episodes, average score is 15.20. Took 14 seconds.\n",
      "After 270 episodes, average score is 28.03. Took 17 seconds.\n",
      "After 280 episodes, average score is 38.67. Took 15 seconds.\n",
      "After 290 episodes, average score is 44.12. Took 13 seconds.\n",
      "After 300 episodes, average score is 52.86. Took 11 seconds.\n",
      "After 310 episodes, average score is 56.11. Took 12 seconds.\n",
      "After 320 episodes, average score is 56.68. Took 6 seconds.\n",
      "After 330 episodes, average score is 76.58. Took 13 seconds.\n",
      "After 340 episodes, average score is 97.72. Took 11 seconds.\n",
      "After 350 episodes, average score is 113.16. Took 9 seconds.\n",
      "After 360 episodes, average score is 118.35. Took 8 seconds.\n",
      "After 370 episodes, average score is 124.56. Took 9 seconds.\n",
      "After 380 episodes, average score is 132.44. Took 8 seconds.\n",
      "After 390 episodes, average score is 142.06. Took 7 seconds.\n",
      "After 400 episodes, average score is 145.98. Took 6 seconds.\n",
      "After 410 episodes, average score is 144.98. Took 6 seconds.\n",
      "After 420 episodes, average score is 161.05. Took 8 seconds.\n",
      "After 430 episodes, average score is 159.94. Took 8 seconds.\n",
      "After 440 episodes, average score is 162.40. Took 7 seconds.\n",
      "After 450 episodes, average score is 154.51. Took 7 seconds.\n",
      "After 460 episodes, average score is 152.51. Took 6 seconds.\n",
      "After 470 episodes, average score is 152.79. Took 7 seconds.\n",
      "After 480 episodes, average score is 150.11. Took 9 seconds.\n",
      "After 490 episodes, average score is 154.44. Took 10 seconds.\n",
      "After 500 episodes, average score is 156.44. Took 8 seconds.\n",
      "After 510 episodes, average score is 164.28. Took 11 seconds.\n",
      "After 520 episodes, average score is 164.50. Took 12 seconds.\n",
      "After 530 episodes, average score is 165.20. Took 13 seconds.\n",
      "After 540 episodes, average score is 157.27. Took 10 seconds.\n",
      "After 550 episodes, average score is 161.78. Took 15 seconds.\n",
      "After 560 episodes, average score is 181.71. Took 7 seconds.\n",
      "After 570 episodes, average score is 191.67. Took 9 seconds.\n",
      "After 580 episodes, average score is 195.85. Took 9 seconds.\n",
      "Environment solved in 589 episodes. Average score: 201.21\n",
      "Took 769 seconds (~12.0 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Get state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(f'State size: {state_size}, action size: {action_size}')\n",
    "dqn_agent = DQNAgent(state_size, action_size)\n",
    "start = time()\n",
    "last_time = start\n",
    "\n",
    "scores_window = deque(maxlen=100)\n",
    "mean_score = 0\n",
    "episode = 0\n",
    "\n",
    "while True:\n",
    "    episode += 1\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(MAX_EPISODE_LENGTH):\n",
    "        \n",
    "        epsilon = max(1/episode, EPSILON_MIN)\n",
    "        action = dqn_agent.choose_action(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        dqn_agent.train(state, action, reward, next_state, done)\n",
    "        state = next_state        \n",
    "        score += reward        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores_window.append(score)\n",
    "    mean_score = np.mean(scores_window)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f'After {episode} episodes, average score is {mean_score:.2f}. ', end='')\n",
    "        print(f'Took {time()-last_time:.0f} seconds.')\n",
    "        last_time = time()\n",
    "    \n",
    "    if mean_score >= TARGET_SCORE:\n",
    "        print(f'Environment solved in {episode} episodes. Average score: {mean_score:.2f}')\n",
    "        break\n",
    "\n",
    "print(f'Took {time()-start:.0f} seconds (~{(time()-start)//60} minutes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd7QnYQRVUFc"
   },
   "source": [
    "### 4. Play epsiode and record it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gki1NW8sVlyd"
   },
   "source": [
    "Use the trained model to play and record one episode. The recorded video will be stored into the `video`-subfolder on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-KWsd02TRZq",
    "outputId": "2b18059f-f9af-4ca7-ff69-cadfb97df3ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 12:54:37.323 python[8293:3025131] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f938e87d640>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-06-13 12:54:37.323 python[8293:3025131] Warning: Expected min height of view: (<NSButton: 0x7f938e893d60>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-06-13 12:54:37.326 python[8293:3025131] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f938e894880>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-06-13 12:54:37.328 python[8293:3025131] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f938c4bd590>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "Unknown encoder 'libx264'\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 215.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder failed: None\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder exited with status 1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "FPS = 25\n",
    "record_folder=\"video\"  \n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "    env.render()\n",
    "\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    action_values = dqn_agent.q_network(state)\n",
    "    action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    compute_time = time.time() - start_time\n",
    "    delta = 1/FPS - compute_time\n",
    "    if delta > 0:\n",
    "        time.sleep(delta)\n",
    "\n",
    "print(f\"Total reward: {total_reward:.2f}\" )\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Task: Implement the following functions to make the code above work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the agent train something, we need to implement the `calculate_loss` function in the code above. To make this easier, we do this along the following mini tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, were are given a tiny replay buffer that contains only two transitions of the form `state`, `action`, `reward`, `next_state` and `done`. \n",
    "\n",
    "The resulting tensors `states`, `actions`, `rewards`, `next_states` and `dones` are of the same format as the input to the function `calculate_loss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = [ 0.64,  0.38,  0.04, -0.10, -0.22, -.00,  0.00,  0.00]\n",
    "state_2 = [ 0.00,  0.35,  0.41, -0.59, -0.66, -0.23,  0.00,  0.00]\n",
    "states = torch.FloatTensor([state_1, state_2])\n",
    "\n",
    "actions = torch.LongTensor([[2],[1]])\n",
    "\n",
    "rewards = torch.FloatTensor([[1.8670],[1.2630]])\n",
    "\n",
    "next_state_1 = [-0.60,  0.94, -0.04, -0.13,  0.27, 0.70,  0.00,  0.00]\n",
    "next_state_2 = [-0.60,  0.94, -0.04, -0.13,  0.27, 0.70,  0.00,  0.00]\n",
    "next_states = torch.FloatTensor([next_state_1, next_state_2])\n",
    "\n",
    "dones = torch.FloatTensor([[0],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first calculate the Q-Learning target. In a first step we use the `target_network` to calculate the Q-values for every state in the `next_states` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49.8446, 44.1981, 52.0392, 56.0594],\n",
       "        [49.8446, 44.1981, 52.0392, 56.0594]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = dqn_agent.target_network(next_states)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not want to backpropagate on these values, we detach them from the computational graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49.8446, 44.1981, 52.0392, 56.0594],\n",
       "        [49.8446, 44.1981, 52.0392, 56.0594]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = q_values.detach()\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using Q-Learning, we are only interested in the maximum value per line.\n",
    "Implement some code that squashed the above to a torch tensor of shape `[2, 1]` that contains for every state only the maximum Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[56.0594],\n",
       "        [56.0594]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_q_values = q_values.max(1)[0].unsqueeze(1)\n",
    "max_q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to calcualte the Q-Learning targets using the tensors `rewards` and `dones` as seen in the lecture. Remember: The target consist only of the reward if the done flag is set for a transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57.3658],\n",
       "        [ 1.2630]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "Q_targets = rewards + (GAMMA * max_q_values * (1 - dones))\n",
    "Q_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now caluclate the predicton of the network on the current states. For this we use the `q_network` of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45.6411, 48.2667, 47.1487, 43.2907],\n",
       "        [ 2.1099,  9.9811,  6.8551, -2.1050]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = dqn_agent.q_network(states)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns for every state the Q-values for all actions. However, we only need the q-values of the according  that was actually taken in this transition (this is stored in `actions`).\n",
    "Next, extract the Q-Value for the taken action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[47.1487],\n",
       "        [ 9.9811]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value_action = predictions.gather(1, actions)\n",
    "q_value_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values can now be used to define the loss for the current batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.mse_loss(q_value_action, Q_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 3:\n",
    "Use the code from these examples to implement the `calculate_loss` function from above and train the agent."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5_DQN_LunarLander.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
