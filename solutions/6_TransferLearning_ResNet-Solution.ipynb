{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe633a53",
   "metadata": {},
   "source": [
    "# Transfer Learning with Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f04e7",
   "metadata": {},
   "source": [
    "In this notebook we load a small datasets that contains dolphins and elephants. We classify the images using CNNs and compare two approaches and see what worsk better:\n",
    "1. Training a CNN from sratch against\n",
    "2. Finetuning a pretrained ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9347627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdd2210f710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ce7b5",
   "metadata": {},
   "source": [
    "Let's load our data a have a look at the shape of some images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0508c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=RGB size=300x179 at 0x7FDD242741F0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x179 at 0x7FDD2427FA90>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x166 at 0x7FDD2437E8E0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x259 at 0x7FDD242741F0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x225 at 0x7FDD2437E280>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x277 at 0x7FDD2427FA90>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x183 at 0x7FDD2437EEE0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x225 at 0x7FDD2427F670>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x214 at 0x7FDD2437E8E0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x223 at 0x7FDD2427FA90>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x277 at 0x7FDD2437E280>, 0)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(root='../data/anmials')\n",
    "for i, data in enumerate(dataset):\n",
    "    print(data)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4e264",
   "metadata": {},
   "source": [
    "We see that the pictures have all width=300 but a varying height. To use them in transfer learning they need to have the standard shape (224, 224), which is the data format of ImageNet (on which most pretrained models are trained on).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb2c83",
   "metadata": {},
   "source": [
    "To get them into this shape, we first increase the height to 224 (this will also increase the height) and then take the 224 square which is center in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551777c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "             transforms.Resize(size=224),\n",
    "             transforms.CenterCrop(size=224),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) # standard normalization for transfer learning\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f869668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 data points\n"
     ]
    }
   ],
   "source": [
    "data = datasets.ImageFolder(root='../data/anmials', transform=image_transforms)\n",
    "\n",
    "print(len(data), \"data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e3626",
   "metadata": {},
   "source": [
    "Next, we split the data into train and test and define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43d1ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(data, [100, 29])\n",
    "\n",
    "batch_size = 10\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=29,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bce70",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "### Task 1.\n",
    "Train a CNN from scratch to identify the object on the image (dolphin or elephant). For this, use the same CNN architecture as in notebook `5_CNN_CIFAR10.ipynb` (to make this work, you need to adjust some network parameters for this dataset). Train for 20 epochs on the train data and afterwards compute accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8fafcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(44944, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314faedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, loss: 6.88152\n",
      "2, loss: 6.62755\n",
      "3, loss: 6.11002\n",
      "4, loss: 5.09790\n",
      "5, loss: 3.92423\n",
      "6, loss: 3.11455\n",
      "7, loss: 2.17701\n",
      "8, loss: 1.75489\n",
      "9, loss: 1.16424\n",
      "10, loss: 0.91531\n",
      "11, loss: 0.78906\n",
      "12, loss: 0.47361\n",
      "13, loss: 0.53425\n",
      "14, loss: 0.29821\n",
      "15, loss: 0.20747\n",
      "16, loss: 0.13045\n",
      "17, loss: 0.06855\n",
      "18, loss: 0.04904\n",
      "19, loss: 0.04147\n",
      "20, loss: 0.03367\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    loss_batch = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_batch += loss.item()\n",
    "        \n",
    "    print(f'{epoch + 1}, loss: {loss_batch:.5f}')\n",
    "    loss_batch = 0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d7edaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 86 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "wrong_images = []\n",
    "wrong_labesl = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd04a89",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "Instead of training a CNN, load a pretrained ResNet18 and only train the last layer (see the lecture slides how that works). Train again for 20 epochs and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db263f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "fc_inputs = model.fc.in_features\n",
    "model.fc = nn.Linear(fc_inputs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c61aa963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, loss: 6.21116\n",
      "2, loss: 3.06055\n",
      "3, loss: 2.19162\n",
      "4, loss: 1.54697\n",
      "5, loss: 1.20638\n",
      "6, loss: 1.26148\n",
      "7, loss: 1.34913\n",
      "8, loss: 0.98523\n",
      "9, loss: 0.76459\n",
      "10, loss: 1.12265\n",
      "11, loss: 0.80007\n",
      "12, loss: 0.80555\n",
      "13, loss: 1.16048\n",
      "14, loss: 1.41405\n",
      "15, loss: 0.62329\n",
      "16, loss: 1.00320\n",
      "17, loss: 1.26670\n",
      "18, loss: 0.60839\n",
      "19, loss: 0.56927\n",
      "20, loss: 0.47967\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    loss_batch = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        #print(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        #print(outputs.shape)\n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_batch += loss.item()\n",
    "        \n",
    "    print(f'{epoch + 1}, loss: {loss_batch:.5f}')\n",
    "    loss_batch = 0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3c5f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "wrong_images = []\n",
    "wrong_labesl = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
