{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCLrRFHSKl_5"
   },
   "source": [
    "# Lunar Lander with Cross-Entropy Method\n",
    "\n",
    "In this notebook we look at the lunar lander environment and solve it with the cross-entropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96dExX1TKm2m",
    "outputId": "59a0cc23-613d-4378-8de6-2b4d280e9fa9"
   },
   "outputs": [],
   "source": [
    "#!pip3 install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CZXskDwXKl_-"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import deque\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhZ0fzBkKmAA"
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "We define a simple neural network that generates the action scores based on a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vWQr7TZgKmAB"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zFMlVViKmAE"
   },
   "source": [
    "# Generate Episodes\n",
    "\n",
    "We generate a batch of episodes and remember the traversed states, actions and rewards. To select the next action we use the output of the network. For this we first pass the scores through a softmax to get probabilites. In the second step we sampel from this distribution to get the next action to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AIiayltZKmAF"
   },
   "outputs": [],
   "source": [
    "def generate_batch(env, batch_size, t_max=5000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states, actions = [], []\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "\n",
    "            new_s, r, done, info = env.step(a)\n",
    "\n",
    "            # record sessions like you did before\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "                \n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvq2ZIvlKmAJ"
   },
   "source": [
    "# Training\n",
    "\n",
    "In the training step, we first use the neural network to generate a batch of episodes and then use the state-action pairs to improve the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFUzEnaDKmAJ",
    "outputId": "a5344f76-e542-4566-808e-8864fcdd4f09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/qd5krtcd4bs0p2_c6rcbbb7c0000gn/T/ipykernel_58211/1046171293.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1646755922314/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  s_v = torch.FloatTensor([s])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.400, reward_mean=-184.0\n",
      "1: loss=1.393, reward_mean=-180.3\n",
      "2: loss=1.381, reward_mean=-172.0\n",
      "3: loss=1.382, reward_mean=-173.5\n",
      "4: loss=1.385, reward_mean=-176.4\n",
      "5: loss=1.403, reward_mean=-172.3\n",
      "6: loss=1.387, reward_mean=-171.5\n",
      "7: loss=1.388, reward_mean=-182.9\n",
      "8: loss=1.381, reward_mean=-171.2\n",
      "9: loss=1.368, reward_mean=-171.4\n",
      "10: loss=1.377, reward_mean=-186.2\n",
      "11: loss=1.382, reward_mean=-177.7\n",
      "12: loss=1.389, reward_mean=-214.7\n",
      "13: loss=1.413, reward_mean=-187.9\n",
      "14: loss=1.376, reward_mean=-210.3\n",
      "15: loss=1.364, reward_mean=-198.9\n",
      "16: loss=1.388, reward_mean=-183.4\n",
      "17: loss=1.373, reward_mean=-212.1\n",
      "18: loss=1.387, reward_mean=-189.1\n",
      "19: loss=1.394, reward_mean=-188.5\n",
      "20: loss=1.405, reward_mean=-193.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g4/qd5krtcd4bs0p2_c6rcbbb7c0000gn/T/ipykernel_58211/281042103.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# TODO-1: here we need to filter out episodes that are not good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/g4/qd5krtcd4bs0p2_c6rcbbb7c0000gn/T/ipykernel_58211/1046171293.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(env, batch_size, t_max)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/dmml2/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/dmml2/lib/python3.8/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m         ]\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         self.moon = self.world.CreateStaticBody(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mshapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medgeShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         )\n",
      "\u001b[0;32m/opt/miniconda3/envs/dmml2/lib/python3.8/site-packages/Box2D/Box2D.py\u001b[0m in \u001b[0;36mCreateStaticBody\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m         \"\"\"\n\u001b[1;32m   3204\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2_staticBody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCreateBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/dmml2/lib/python3.8/site-packages/Box2D/Box2D.py\u001b[0m in \u001b[0;36mCreateBody\u001b[0;34m(self, defn, **kwargs)\u001b[0m\n\u001b[1;32m   3245\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3247\u001b[0;31m             \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFixturesFromShapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapeFixture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshapeFixture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'massData'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/dmml2/lib/python3.8/site-packages/Box2D/Box2D.py\u001b[0m in \u001b[0;36mCreateFixturesFromShapes\u001b[0;34m(self, shapes, shapeFixture)\u001b[0m\n\u001b[1;32m   2529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshapeFixture\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m             \u001b[0mshapeFixture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb2FixtureDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m             \u001b[0moldShape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/dmml2/lib/python3.8/site-packages/Box2D/Box2D.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0m_Box2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2FixtureDef_swiginit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_Box2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_b2FixtureDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0m_init_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "session_size = 100\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "completion_score = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(session_size):\n",
    "    \n",
    "    # generate new episodes\n",
    "    states, actions, rewards = generate_batch(env, batch_size, t_max=500)\n",
    "\n",
    "    # TODO-1: here we need to filter out episodes that are not good\n",
    "    \n",
    "\n",
    "    # train on the states using actions as targets\n",
    "    for s_i in range(len(states)):\n",
    "        optimizer.zero_grad()\n",
    "        tensor_states = torch.FloatTensor(states[s_i])\n",
    "        tensor_actions = torch.LongTensor(actions[s_i])\n",
    "        action_scores_v = net(tensor_states)\n",
    "        loss_v = objective(action_scores_v, tensor_actions)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward = np.mean(rewards)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f\" % (\n",
    "            i, loss_v.item(), mean_reward))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg_Cz4SwKJJm"
   },
   "source": [
    "Use the trained model to play and record one episode. The recorded video will be stored into the video-subfolder on disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2KaeLMD9Dx7"
   },
   "outputs": [],
   "source": [
    "# TODO-2: Play episode with agent and record it!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "6_LunarLander_PolicyBased.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
