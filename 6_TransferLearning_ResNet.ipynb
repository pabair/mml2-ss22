{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe633a53",
   "metadata": {},
   "source": [
    "# Transfer Learning with Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f04e7",
   "metadata": {},
   "source": [
    "In this notebook we load a small datasets that contains dolphins and elephants. We classify the images using CNNs and compare two approaches and see what worsk better:\n",
    "1. Training a CNN from sratch against\n",
    "2. Finetuning a pretrained ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9347627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc76ca13710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ce7b5",
   "metadata": {},
   "source": [
    "Let's load our data a have a look at the shape of some images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0508c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=RGB size=300x179 at 0x7FC76ED10160>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x179 at 0x7FC76C2A2250>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x166 at 0x7FC76ED10220>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x259 at 0x7FC76C2A2610>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x225 at 0x7FC76ED101F0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x277 at 0x7FC76EBEDCA0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x183 at 0x7FC76ED10160>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x225 at 0x7FC76EBEDCA0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x214 at 0x7FC76ED10280>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x223 at 0x7FC76EBEDCA0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x277 at 0x7FC76C273E20>, 0)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(root='./data/anmials')\n",
    "for i, data in enumerate(dataset):\n",
    "    print(data)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4e264",
   "metadata": {},
   "source": [
    "We see that the pictures have all width=300 but a varying height. To use them in transfer learning they need to have the standard shape (224, 224), which is the data format of ImageNet (on which most pretrained models are trained on).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb2c83",
   "metadata": {},
   "source": [
    "To get them into this shape, we first increase the height to 224 (this will also increase the height) and then take the 224 square which is center in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551777c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "             transforms.Resize(size=224),\n",
    "             transforms.CenterCrop(size=224),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) # standard normalization for transfer learning\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f869668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 data points\n"
     ]
    }
   ],
   "source": [
    "data = datasets.ImageFolder(root='./data/anmials', transform=image_transforms)\n",
    "\n",
    "print(len(data), \"data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e3626",
   "metadata": {},
   "source": [
    "Next, we split the data into train and test and define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d1ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(data, [100, 29])\n",
    "\n",
    "batch_size = 10\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=29,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bce70",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "### Task 1.\n",
    "Train a CNN from scratch to identify the object on the image (dolphin or elephant). For this, use the same CNN architecture as in notebook `5_CNN_CIFAR10.ipynb` (to make this work, you need to adjust some network parameters for this dataset). Train for 20 epochs on the train data and afterwards compute accuracy on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd04a89",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "Instead of training a CNN, load a pretrained ResNet18 and only train the last layer (see the lecture slides how that works). Train again for 20 epochs and compare the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
